\pagebreak
\section{G3: Design for Progressive Scalability}
\label{sec:g3-design-progressive-scalability}

This section presents G3, which focuses on designing modular monoliths so that scalability is achieved progressively rather than through premature distribution. In this dissertation, \emph{progressive scalability} is defined as the capacity of a modular monolith to accommodate increasing load, data volume, and organizational growth through a deliberate sequence of architectural interventions, each proportional to the observed need, without requiring a wholesale migration to a distributed architecture. G3 operationalizes this capacity by identifying module-level scale points, introducing asynchronous boundaries where load demands diverge, and abstracting infrastructure dependencies so that extraction remains a controlled option rather than an emergency.

G3 completes the Architectural Design Dimension (G1--G3). While G1 enforces modular boundaries and G2 preserves maintainability through bounded cost of change, G3 ensures that the resulting modular structure can absorb growth without collapsing into either a monolithic bottleneck or a premature microservices decomposition. Together, these three guidelines establish the structural foundation upon which the Operational Fit, Organizational Alignment, and Guideline Orientation dimensions build.

\subsection*{Intent and Rationale}

The prevailing narrative in cloud-native literature frames scalability as an inherently distributed concern: when the system must scale, it must be decomposed into independently deployable services~\cite{arya2024beyond, blinowski2022monolithic}. This framing creates a false dichotomy. Teams are implicitly told to choose between a monolith that cannot scale and microservices that can, ignoring the intermediate architectural states that a well-modularized monolith can occupy.

In practice, most early-stage systems do not face uniform scaling pressure. Load concentrates in specific modules, specific use cases exhibit disproportionate growth, and specific data domains expand faster than others. A system designed for progressive scalability acknowledges this unevenness and provides mechanisms to address it surgically, scaling the modules that need it while leaving the rest undisturbed.

G3 therefore treats scalability as a \emph{gradient} rather than a binary switch. The guideline defines a four-level Progressive Scalability Spectrum (Section~\ref{tab:scalability-spectrum}) that progresses from vertical optimization within the monolith, through asynchronous decoupling and data isolation, to selective extraction of bounded contexts as a last resort. Each level is triggered by observable metrics rather than speculative anticipation, and each intervention is reversible if conditions change.

This spectrum makes the migration to microservices an \emph{option}, not an obligation. Each step is proportional to observed need and grounded in the boundary enforcement (G1) and maintainability discipline (G2) already established. The key insight is that a system with enforced modular boundaries and stable contracts is \emph{already extraction-ready}; what G3 adds is the deliberate preparation of infrastructure seams, established as an initial architectural decision, that make extraction low-risk when evidence warrants it.

\subsection*{Conceptual Overview}

Progressive scalability is embedded by designing modules so that growth is absorbed incrementally. The key architectural decision is that both synchronous and asynchronous communication channels are available as part of the system's foundational design: synchronous calls serve low-latency, consistency-critical paths, while asynchronous channels (event buses, message queues) absorb throughput divergence. The choice between channels is driven by observed load patterns, not by speculative anticipation, and the transition from one to the other is a configuration change rather than a rewrite.

Critically, scalability readiness must be \emph{automatically tracked}, not manually assessed. Module-level metrics (throughput profiles, latency variance, data ownership clarity) are collected continuously and evaluated against defined thresholds. When a metric crosses its threshold, the system signals that a specific module requires a specific intervention at a specific spectrum level, removing subjectivity from scaling decisions.

\begin{itemize}[itemsep=8pt,topsep=2pt]
  \item Each module's resource consumption, throughput profile, and data growth trajectory are observable and attributable, enabling evidence-based scaling decisions.
  \item Communication between modules with divergent scaling needs can transition from synchronous to asynchronous without rewriting business logic.
  \item Persistence is designed so that schema ownership can be narrowed from shared to module-scoped without data migration crises.
  \item Infrastructure dependencies (databases, caches, queues, external APIs) are accessed through abstractions that decouple business logic from deployment topology.
\end{itemize}

\subsection*{Applicability Conditions and Scope}

G3 applies to modular monolith systems that satisfy the following conditions:
\begin{itemize}[itemsep=8pt,topsep=2pt]
  \item Module boundaries are enforced and dependencies are explicit, as established by G1.
  \item Maintainability discipline is in place, ensuring that contracts are stable and change locality is preserved, as established by G2.
  \item The system is expected to grow in load, data volume, or team size, but the timing and distribution of that growth are uncertain.
\end{itemize}

G3 does not prescribe specific cloud providers, container orchestrators, or scaling technologies. Its scope is limited to architectural preparation: the design decisions and structural properties that make scaling interventions feasible, proportional, and reversible. The guideline is agnostic to whether the system ultimately remains a monolith, becomes a set of microservices, or stabilizes at an intermediate state. G3's scalability metrics, particularly the extraction readiness score ($\varepsilon_m$), provide the quantitative trigger for G4 (Migration Readiness), which addresses how to prepare a specific module for independent deployment once evidence warrants it.


\subsection*{Objectives}

\emph{Research contributions:}
\begin{itemize}[itemsep=8pt,topsep=2pt]
  \item \emph{Formalize scalability as a gradient:} Define a progression spectrum (L0--L3) that replaces the binary monolith-or-microservices framing with a sequence of proportional, reversible interventions, each justified by evidence rather than anticipation.
  \item \emph{Quantify progression triggers:} Establish module-level metrics ($\Delta\Theta$, $\rho_{\mathrm{sync}}$, $\omega$, $\alpha$) that determine when a specific module should transition from one spectrum level to the next, removing subjectivity from scaling decisions.
  \item \emph{Provide a composite extraction readiness metric:} Synthesize boundary health (G1), maintainability signals (G2), and scalability preparation (G3) into a per-module extraction readiness score ($\varepsilon_m$) that grounds L3 decisions in architectural evidence.
\end{itemize}

\emph{Implementation objectives:}
\begin{itemize}[itemsep=8pt,topsep=2pt]
  \item \emph{Embed scalability infrastructure from inception:} Design the architecture so that the infrastructure for all spectrum levels (caching, async channels, schema isolation) is present in the initial deployment, making progression a configuration change rather than a migration project.
  \item \emph{Abstract infrastructure as a replaceable dependency:} Ensure modules interact with infrastructure through ports or interfaces that can be re-bound across spectrum levels without modifying domain logic.
  \item \emph{Preserve extraction optionality:} Maintain the ability to extract any bounded context as an independent service while positioning extraction as the last resort, not the first reflex.
  \item \emph{Record progression decisions:} Document each spectrum-level transition as an Architecture Decision Record (ADR) that captures the trigger condition, the intervention applied, the expected outcome, and the rollback path.
\end{itemize}


\subsection*{Key Principles}

\begin{itemize}[itemsep=8pt,topsep=2pt]
  \item \emph{Scale where it hurts, not where it might:}
  Scaling interventions should be driven by observed bottlenecks, not by speculative anticipation. Module-level metrics (see Metrics section) provide the evidence base. Premature optimization of modules that are not under pressure introduces unnecessary complexity and operational cost.

  \item \emph{Asynchronous boundaries as scalability seams:}
  The boundary between two modules becomes a scalability seam when their throughput requirements diverge. Introducing an asynchronous channel (event bus, message queue) at this seam absorbs load spikes in the producer without propagating backpressure to the consumer, and vice versa. The key constraint is that asynchronous boundaries must respect the contracts established in G1 and the stability properties maintained by G2.

  \item \emph{Schema isolation is established from project inception:}
  Each module owns a dedicated PostgreSQL schema established at project start. Cross-module data access is mediated exclusively through published contracts (events or API calls), never through direct table queries. This design decision, made as an initial architectural decision rather than deferred to a future migration, ensures that data isolation is a structural property of the system rather than a prerequisite to be satisfied before extraction.

  \item \emph{Infrastructure as a replaceable dependency:}
  Modules should depend on infrastructure through ports (interfaces) rather than concrete implementations. This is not an abstract design preference but a scalability prerequisite: when a module must move from an in-process event bus to a distributed message broker, or from a shared PostgreSQL instance to a dedicated read replica, the change should be confined to the infrastructure adapter, not the domain logic.

  \item \emph{Extraction is the last resort, not the first reflex:}
  Extracting a module into an independent service introduces network boundaries, distributed failure modes, deployment complexity, and operational overhead. G3 positions extraction at the end of the scalability spectrum, after vertical optimization, async decoupling, and data isolation have been exhausted or proven insufficient. This ordering preserves system simplicity for as long as possible while ensuring that extraction remains feasible when genuinely needed.
\end{itemize}


\pagebreak
\subsection*{Implementation Mechanisms}

G3 is implemented through the following architectural mechanisms, all established at project start:

\begin{itemize}[itemsep=8pt,topsep=2pt]
  \item \emph{Module-scoped resource profiling:} Each module's resource consumption (CPU, memory, I/O, query volume) is attributable through instrumentation or naming conventions, enabling identification of scale points without distributed tracing.

  \item \emph{Communication abstraction layer:} Operationalizing the \emph{asynchronous boundaries as scalability seams} principle (above), inter-module communication passes through a contract-based abstraction (e.g., an event bus interface or a command dispatcher) that can be backed by in-process dispatch, an in-memory queue, or a distributed broker without changing the caller or handler.

  \item \emph{Persistence port pattern:} Operationalizing \emph{schema isolation from inception} and \emph{infrastructure as a replaceable dependency}, each module accesses its data through repository interfaces (ports) backed by a dedicated PostgreSQL schema established at project start. The concrete implementation is injected at composition time; when a module is extracted (L3), the repository adapter is re-bound to a dedicated database instance without changing domain logic. Schema ownership is documented in the module descriptor introduced in G1.

  \item \emph{Scalability decision records:} When a module transitions between spectrum levels (e.g., from L0 to L1), the decision is recorded in an Architecture Decision Record (ADR) that captures the trigger condition, the intervention applied, the expected outcome, and the rollback path. This ensures that scaling decisions remain traceable and reversible.
\end{itemize}


\subsubsection*{The Progressive Scalability Spectrum}

To make the gradient concrete, G3 proposes a four-level spectrum that teams can use to assess their current position and plan their next proportional intervention. Each level builds on the previous one, and progression is driven by evidence rather than anticipation.

\begin{table}[H]
\centering
\small
\caption{Progressive Scalability Spectrum for Modular Monoliths}
\label{tab:scalability-spectrum}
\rowcolors{2}{gray!5}{white}
\begin{tabularx}{\linewidth}{p{0.06\linewidth} p{0.18\linewidth} X p{0.22\linewidth}}
\toprule
\textbf{Level} & \textbf{Intervention} & \textbf{Description} & \textbf{Trigger Condition} \\
\midrule
L0 & Vertical Optimization & Resource tuning, caching, query optimization, concurrency improvements within the monolith. No structural change. & Module-level latency or throughput degradation detected. \\
L1 & Async Decoupling & Replace synchronous inter-module calls with asynchronous channels (event bus, message queue) for modules with divergent throughput. & Synchronous calls between modules create backpressure or latency spikes under load. \\
L2 & Data Isolation & Separate module-scoped persistence (dedicated schemas, read replicas, or separate databases) for modules with divergent data access patterns. & Shared database contention, conflicting consistency requirements, or schema evolution friction. \\
L3 & Selective Extraction & Deploy a bounded context as an independent service with its own runtime, persistence, and deployment pipeline. & L0--L2 exhausted; module requires independent scaling, independent release cadence, or technology heterogeneity. \\
\bottomrule
\end{tabularx}
\end{table}

\noindent
The spectrum is not strictly linear: a team may apply L2 (data isolation) to one module while another module remains at L0. The key constraint is that each intervention at a given level should be justified by evidence that the previous level is insufficient for the module in question.


\subsection*{Common Failure Modes and Anti-Patterns}

The following failure modes are frequently observed in modular monoliths that lack explicit scalability preparation. Each anti-pattern increases the risk of premature or disproportionate scaling interventions. For each anti-pattern, G3 identifies the specific metric that makes the degradation observable and verifiable.

\begin{itemize}[itemsep=8pt,topsep=2pt]
  \item \emph{Premature Distribution (``Microservices Envy''):}
  Extracting modules into services before exhausting in-process optimizations. This introduces distributed-system complexity (network latency, partial failures, eventual consistency) without evidence that distribution is necessary. The cost is disproportionate to the benefit, particularly for early-stage teams with limited operational maturity~\cite{gravanis2021dont, su2024from}. \textbf{Metric signal:} detectable when $\varepsilon_m$ thresholds are used as extraction triggers before L0--L2 interventions have been exhausted; a premature L3 decision despite low $\Delta\Theta$ divergence confirms this anti-pattern.

  \item \emph{Uniform Scaling Assumption:}
  Treating all modules as having identical scaling requirements and applying the same intervention (e.g., horizontal replication of the entire monolith) uniformly. This wastes resources on modules that are not under pressure and delays targeted intervention for the modules that are. \textbf{Metric signal:} visible when $\Delta\Theta$ shows divergence across modules but the scaling intervention is applied uniformly rather than targeted to the modules with the highest throughput variance.

  \item \emph{Shared Database as Implicit Coupling:}
  Allowing modules to query each other's tables directly, creating an invisible dependency that prevents data isolation (L2) and makes extraction (L3) require a coordinated schema migration. This anti-pattern often accumulates silently because it does not violate code-level boundary checks. \textbf{Metric signal:} directly detected by the data ownership clarity index: $\omega < 1.0$ signals that at least one table is accessed by multiple modules.

  \item \emph{Synchronous Call Chains Under Load:}
  Maintaining synchronous inter-module communication paths for workflows that exhibit high throughput variance. Under load, synchronous chains propagate latency and failures upstream, creating cascading degradation that appears as a system-wide outage rather than a localized bottleneck. \textbf{Metric signal:} captured by $\rho_{\mathrm{sync}} > 0.3$, indicating that a disproportionate share of inter-module communication remains synchronous.

  \item \emph{Infrastructure Lock-In Through Concrete Dependencies:}
  Embedding infrastructure-specific code (e.g., direct database driver calls, cloud SDK invocations) in domain or application logic. This prevents the module from transitioning between spectrum levels without rewriting business logic, effectively freezing the architecture at its current scale point. \textbf{Metric signal:} detected by $\alpha < 1.0$, indicating that at least one module bypasses the shared infrastructure abstraction layer.
\end{itemize}

These anti-patterns emerge when scaling decisions are driven by anticipation rather than evidence. G3 addresses them by embedding infrastructure abstractions as part of the system's foundational design, tracking scalability metrics longitudinally, and requiring evidence-based justification for each spectrum-level transition.

\subsubsection*{The Scalability Readiness Gap}

G1 and G2 ensure that module boundaries are correct and sustainable. Yet a system can satisfy both guidelines and still be \emph{unable to scale proportionally}. The gap arises because boundary enforcement and maintainability tracking say nothing about whether the architecture is \emph{prepared} for scaling interventions. A module with perfect boundary discipline and stable contracts may still share a database schema with three other modules, communicate exclusively through synchronous calls, and depend on infrastructure through concrete implementations. When load arrives, the team discovers that scaling requires a coordinated migration rather than a surgical intervention.

This is the Scalability Readiness Gap: the distance between a structurally sound modular monolith and one that can absorb growth through proportional, reversible interventions. G3's metrics close this gap by making infrastructure preparation, communication flexibility, and data ownership visible and measurable from the initial deployment.


\pagebreak
\subsection*{Metrics and Verification}

G3 metrics are designed to make scaling pressure observable at the module level and to verify that the architectural preparation for progressive scalability is in place. They complement the structural metrics defined in G1 and G2 by adding resource, throughput, and isolation dimensions.

\subsubsection*{Runtime Metrics}

\begin{itemize}[itemsep=8pt,topsep=2pt]
  \item \textbf{Module Throughput Profile ($\Theta_m$).} \\
  Quantifies throughput divergence across modules to identify candidates for asynchronous decoupling before bottlenecks manifest as user-visible latency~\cite{richardson2018microservices}.
  \[
    \Delta\Theta = \max_{m \in M} \Theta_m - \min_{m \in M} \Theta_m
  \]
  \emph{Target:} Monitor $\Delta\Theta$ growth; trigger L1 async decoupling when divergence exceeds 2:1 ratio between connected modules.

  \item \textbf{Module Latency Variance ($\sigma^2_{L,m}$).} \\
  Measures response time consistency within a module. High variance under load signals resource contention, inefficient queries, or synchronous dependencies on overloaded modules, making the module a candidate for L0 vertical optimization or L1 async decoupling~\cite{martin2012clean}.
  \[
    \sigma^2_{L,m} = \mathbb{E}[(L_m - \mu_{L,m})^2]
  \]
  \emph{Target:} Monitor $\sigma^2_{L,m}$ growth; investigate when variance increases $>2\times$ baseline under load.

\end{itemize}

\noindent\emph{Note on runtime metrics.} $\Delta\Theta$ and $\sigma^2_{L,m}$ are \emph{monitoring indicators} whose baselines are established through load testing rather than static code analysis. They serve as L0$\to$L1 trigger signals: when throughput divergence between connected modules exceeds a 2:1 ratio, the team investigates async decoupling. The four static and composite metrics below ($\rho_{\mathrm{sync}}$, $\omega$, $\alpha$, $\varepsilon_m$) are the \emph{verification metrics} that can be computed from the codebase at any commit and are included in the compliance summary. The runtime indicators are tracked through the observability infrastructure defined in G6.

\subsubsection*{Static Structural Metrics}

\begin{itemize}[itemsep=8pt,topsep=2pt]
  \item \textbf{Cross-Module Synchronous Call Ratio ($\rho_{\mathrm{sync}}$).} \\
  Quantifies the proportion of synchronous versus asynchronous inter-module interactions. Excessive synchronous communication prevents modules from scaling independently and creates cascading failure scenarios~\cite{dragoni2017microservices}.
  \[
    \rho_{\mathrm{sync}} = \frac{|I_{\mathrm{sync}}|}{|I_{\mathrm{sync}}| + |I_{\mathrm{async}}|}
  \]
  \emph{Target:} $\rho_{\mathrm{sync}} \leq 0.3$ (majority of inter-module communication asynchronous).

  \item \textbf{Data Ownership Clarity Index ($\omega$).} \\
  Measures the proportion of data structures with unambiguous single-module ownership. Shared tables create invisible coupling that prevents data isolation (L2) and blocks extraction (L3)~\cite{evans2003ddd}.
  \[
    \omega = \frac{|T_{\mathrm{single\text{-}owner}}|}{|T|}
  \]
  where $T$ is the set of all tables and $T_{\mathrm{single\text{-}owner}}$ is the subset accessed by only one module.
  \emph{Target:} $\omega = 1.0$ (all data structures have unambiguous ownership).

  \item \textbf{Infrastructure Abstraction Coverage ($\alpha$).} \\
  Quantifies the proportion of infrastructure access mediated through ports or interfaces. Direct infrastructure dependencies freeze the architecture at its current scale point, making spectrum-level transitions require code rewrites rather than configuration changes~\cite{FordParsons2017}.
  \[
    \alpha = \frac{|A_{\mathrm{abstracted}}|}{|A|}
  \]
  \emph{Target:} $\alpha = 1.0$ (all infrastructure access through abstractions).

\end{itemize}

\subsubsection*{Composite Metric}

\begin{itemize}[itemsep=8pt,topsep=2pt]
  \item \textbf{Extraction Readiness Score ($\varepsilon_m$).} \\
  Composite metric synthesizing boundary health (G1), maintainability (G2), and scalability preparation (G3) to quantify how prepared a module is for independent deployment. A high score indicates extraction would be a low-risk topology change; a low score reveals preparation debt~\cite{wolfart2021modernizing}. The weights reflect the relative criticality of each dimension for extraction feasibility:
  \[
    \varepsilon_m = 0.25 \cdot b_m + 0.20 \cdot (1 - \rho_{\mathrm{sync}}^{(m)}) + 0.15 \cdot a_m + 0.20 \cdot \omega_m + 0.20 \cdot v_m
  \]
  where:
  \begin{itemize}[itemsep=2pt,topsep=2pt]
    \item $b_m$ is \emph{boundary compliance}: 1 if module $m$ has no undeclared cross-module imports, 0 otherwise (from G1);
    \item $(1 - \rho_{\mathrm{sync}}^{(m)})$ is \emph{async readiness}: the proportion of inter-module interactions that are event-driven;
    \item $a_m$ is \emph{ACL coverage}: the proportion of cross-module interactions mediated through anti-corruption layers (from G2);
    \item $\omega_m$ is \emph{database schema isolation}: as defined above;
    \item $v_m$ is \emph{event contract versioning coverage}: the proportion of published events with explicit version schemas.
  \end{itemize}
  \emph{Target:} $\varepsilon_m \geq 0.8$ before considering L3 extraction for module $m$.

  Boundary compliance receives the highest weight (0.25) because undeclared cross-module imports create hard extraction blockers that require code changes in both the extracted module and its consumers. Async readiness and data ownership share the second tier (0.20 each) because both represent infrastructure-level coupling that, if unresolved, forces coordinated migration across multiple modules. Event contract versioning (0.20) ensures that extracted modules can communicate through stable schemas without tight temporal coupling. ACL coverage receives the lowest weight (0.15) because anti-corruption layers, while valuable for translation isolation, can be introduced incrementally during extraction without blocking the process. These weights are proposed defaults; teams should calibrate them based on their system's specific coupling profile and extraction priorities.
\end{itemize}

\noindent
\emph{Verification strategy:} G3 metrics are tracked longitudinally alongside G1 and G2 metrics. The primary verification concern is not whether the system scales today, but whether it \emph{can} scale proportionally when needed. An increase in $\Delta\Theta$ without a corresponding decrease in $\rho_{\mathrm{sync}}$ for the affected module pair indicates that scalability preparation is lagging behind actual load growth. A declining $\omega$ indicates increasing data coupling that will block future isolation. These signals enable proactive architectural intervention before scaling becomes an emergency.

\medskip
\noindent
\textbf{Compliance summary.} G3 compliance is defined over the four verifiable metrics:
\[
  \rho_{\mathrm{sync}} \leq 0.3 \land \omega = 1 \land \alpha = 1 \land \varepsilon_m \geq 0.8 \text{ for extraction candidates}
\]


\subsection*{Documentation Guidelines}
\begin{itemize}[itemsep=8pt,topsep=2pt]
  \item \emph{Module Scale Profile:}
  Maintain a lightweight document (or section in the module descriptor) for each module that records its current spectrum level (L0--L3), its observed throughput profile, and any known scaling constraints. This profile is updated when monitoring data changes significantly or when a spectrum-level transition occurs.

  \item \emph{Scalability ADRs:}
  Record each transition between spectrum levels as an Architecture Decision Record. The ADR should capture: the trigger condition (which metric crossed which threshold), the intervention applied, the expected outcome, the rollback path, and the actual outcome after implementation. This creates a traceable history of scaling decisions that can inform future interventions.

  \item \emph{Infrastructure Dependency Map:}
  Document which infrastructure dependencies each module uses and whether they are accessed through abstractions ($\alpha = 1$) or directly ($\alpha < 1$). This map complements the module dependency descriptor from G1 and makes infrastructure lock-in visible.
\end{itemize}

\subsection*{Tooling Capabilities Checklist}
Any open-source or proprietary tool used to support progressive scalability should address:
\begin{itemize}[itemsep=8pt,topsep=2pt]
  \item \emph{Module-scoped metrics collection:} Attribute resource consumption, latency, and throughput to individual modules within the monolith, without requiring distributed tracing infrastructure.
  \item \emph{Communication abstraction:} Provide a dispatch mechanism that supports both synchronous and asynchronous inter-module communication through the same contract interface.
  \item \emph{Schema ownership analysis:} Identify tables or collections accessed by multiple modules and quantify data ownership clarity ($\omega$).
  \item \emph{Load simulation:} Support targeted load testing of individual modules or inter-module communication paths to validate scaling interventions before production deployment.
  \item \emph{Extraction readiness assessment:} Combine G1 boundary metrics, G2 maintainability metrics, and G3 scalability metrics into a composite readiness view per module.
\end{itemize}


\subsection*{Reference Implementation}

The code listings in this section are drawn from the Tiny Store reference implementation. The relevant source files are:

\begin{itemize}[itemsep=8pt,topsep=2pt]
  \item \texttt{libs/shared/infrastructure/src/cache/cache.service.ts}: Module-namespaced caching with adapter pattern (L0)
  \item \texttt{libs/shared/infrastructure/src/cache/cache.decorator.ts}: Read-through caching decorator (L0)
  \item \texttt{libs/shared/infrastructure/src/queue/queue.service.ts}: Module-scoped queues with adapter pattern (L1)
  \item \texttt{libs/modules/shipments/src/jobs/generate-label.job.ts}: Async label generation via queue (L1)
  \item \texttt{libs/shared/infrastructure/src/database/schema-isolation.ts}: Per-module PostgreSQL schemas (L2)
  \item \texttt{tools/metrics/extraction-readiness.ts}: Extraction readiness score calculator ($\varepsilon_m$)
\end{itemize}


\subsection*{Literature Support Commentary}

The academic literature on software scalability exhibits a persistent structural gap. Empirical studies on microservices consistently provide evidence on horizontal scaling but frame distribution as a prerequisite for scalability rather than one option among several~\cite{arya2024beyond, blinowski2022monolithic, berry2024isItWorth}. Comparative analyses of performance, scalability, and cost between microservice and monolithic deployments demonstrate that the trade-offs are context-dependent, yet stop short of proposing what a team should do \emph{between} these two endpoints~\cite{jatkiewicz2023differences}. Studies that advocate for monoliths or modular monoliths acknowledge that monolithic architectures can serve moderate scaling needs but do not formalize how scaling should be approached incrementally within a monolithic boundary~\cite{montesi2021sliceable, gravanis2021dont, deLauretis2019from}. Industry experience reports reinforce this pattern: documented retreats from microservices~\cite{segment2023}, accounts of premature distribution's operational toll~\cite{auth02019}, and arguments that monoliths remain viable at significant scale~\cite{medium2019} all suggest that distribution is not a universal solution. Research on accelerating early-stage development through monolithic architectures combined with MVP practices further supports deferring distribution until evidence warrants it~\cite{lima2024accelerating}.

Two proposals come closest to the progressive scalability concept. The ``sliceable monolith'' keeps the system unified until a slice needs independent deployment~\cite{montesi2021sliceable}, but focuses on \emph{deployment granularity} (when to slice) rather than on the intermediate architectural interventions (async boundaries, data isolation, infrastructure abstraction) that precede extraction. The Strangler Fig pattern~\cite{stranglerPatterns} offers an established incremental migration strategy, but it operates at the application boundary (routing external requests to old or new implementations) rather than addressing the internal modular preparation that precedes extraction. Similarly, the ``write as a monolith, deploy as microservices'' model addresses the deployment dimension but provides no guidance on how to prepare the monolith's internal architecture for selective scaling~\cite{ghemawat2023towards}. The evolutionary architecture perspective provides the philosophical foundation, arguing that architectural properties should be preserved through fitness functions and incremental adaptation~\cite{FordParsons2017}, but does not operationalize this principle for scaling decisions at the module level.

\emph{The gap is threefold.} A systematic review of the SLR corpus reveals that no existing work provides:

\begin{enumerate}[itemsep=8pt,topsep=2pt]
  \item \emph{A formalized intermediate-state model} that defines the architectural states a modular monolith can occupy between ``monolith'' and ``microservices,'' with explicit descriptions of what each state entails structurally.
  \item \emph{Quantified progression triggers} that specify, through measurable metrics ($\rho_{\mathrm{sync}}$, $\omega$, $\alpha$, $\Delta\Theta$), when a module should transition from one scaling level to the next, replacing intuition-driven decisions with evidence-based ones.
  \item \emph{A composite extraction readiness metric} ($\varepsilon_m$) that synthesizes boundary health (G1), maintainability signals (G2), and scalability preparation (G3) into a single actionable score per module, ensuring that extraction decisions are grounded in architectural evidence rather than organizational pressure.
\end{enumerate}

G3 addresses all three gaps. The L0--L3 Progressive Scalability Spectrum (Table~\ref{tab:scalability-spectrum}) formalizes the intermediate states. The six G3 metrics define the progression triggers. The Extraction Readiness Score provides the composite decision function. \medskip
\noindent
\textbf{Core contribution.} G3's contribution is not the individual techniques at each level---caching, async queues, schema isolation are well-established patterns---but the \emph{structured ordering}, the \emph{metric-driven trigger conditions}, and the \emph{verification framework} that ties them into a coherent, evidence-based progression model. This reframes scalability from a binary migration decision into a gradient of proportional, reversible interventions.
\medskip

The scalability levels defined here provide the trigger criteria for G4 (Migration Readiness), which prepares individual modules for extraction when the evidence warrants it.


%% ============================================================
%% G3 APPLIED
%% ============================================================
\pagebreak
\subsection*{G3 Applied: Progressive Scalability in the Tiny Store}
\label{sec:g3-applied}

The Tiny Store reference implementation demonstrates that progressive scalability infrastructure is embedded as an initial architectural requirement, not added later as a scaling patch. The following code listings show the actual production code that ships with the initial deployment.

\paragraph*{L0: Vertical Optimization --- Module-Scoped Caching}

Tiny Store includes a caching layer from the first deployment, not as an optimization added under pressure. The \texttt{CacheService} enforces module-namespaced keys through a \texttt{buildKey()} method, ensuring that cache entries are isolated per bounded context and can be invalidated independently. The service uses an adapter pattern: an \texttt{InMemoryCacheAdapter} serves development and testing, while a \texttt{RedisCacheAdapter} is injected for production workloads.

\begin{lstlisting}[language=TypeScript,caption={CacheService with module-namespaced keys and adapter pattern (cache.service.ts)},label={lst:g3-cache-service}]
class CacheService {
  private adapter: CacheAdapter;
  private globalPrefix: string; // default: 'tiny-store'

  private buildKey(module: string, key: string): string {
    return `${this.globalPrefix}:${module}:${key}`;
  }

  async get<T>(module: string, key: string): Promise<T | null> {
    const raw = await this.adapter.get(
      this.buildKey(module, key));
    if (raw === null) return null;
    return JSON.parse(raw) as T;
  }

  async set<T>(module: string, key: string, value: T,
               ttlSeconds?: number): Promise<void> {
    await this.adapter.set(
      this.buildKey(module, key),
      JSON.stringify(value), ttlSeconds);
  }

  async invalidatePattern(module: string,
                          pattern: string): Promise<void> {
    const keys = await this.adapter.keys(
      this.buildKey(module, pattern));
    if (keys.length > 0) await this.adapter.del(keys);
  }
}
\end{lstlisting}

\noindent
A read-through caching decorator applies this pattern declaratively to any module method:

\begin{lstlisting}[language=TypeScript,caption={Read-through caching decorator (cache.decorator.ts)},label={lst:g3-cache-decorator}]
export function Cacheable(
  module: string, ttlSeconds: number,
  keyFn: (...args: any[]) => string,
) {
  return function (_target: any, _propertyKey: string,
                   descriptor: PropertyDescriptor) {
    const original = descriptor.value;
    descriptor.value = async function (...args: any[]) {
      const cache = CacheService.getInstance();
      const key = keyFn(...args);
      const cached = await cache.get(module, key);
      if (cached !== null) return cached;
      const result = await original.apply(this, args);
      await cache.set(module, key, result, ttlSeconds);
      return result;
    };
    return descriptor;
  };
}
// Usage: @Cacheable('inventory', 60, (sku) => `product:${sku}`)
\end{lstlisting}

\noindent
The key design decision is that the cache layer is present in the initial deployment. When vertical optimization becomes necessary (L0 trigger), the team enables caching on specific queries by adding a decorator; no infrastructure provisioning or architectural change is required.

\paragraph*{L1: Async Decoupling --- Module-Scoped Queues}

Asynchronous processing is an architectural requirement, not a scaling patch. The \texttt{QueueService} provides module-scoped queues following the convention \texttt{module:purpose}. Like the cache layer, it uses an adapter pattern: an \texttt{InMemoryQueueAdapter} handles development, while a \texttt{BullMQAdapter} backed by Redis is injected for production:

\begin{lstlisting}[language=TypeScript,caption={QueueService with module-scoped queues and adapter pattern (queue.service.ts)},label={lst:g3-queue-service}]
class QueueService {
  private adapter: QueueAdapter;

  /** Convention: `module:purpose` e.g. `shipments:label-gen` */
  async enqueue<T>(queue: string, data: T,
                   options?: JobOptions): Promise<Job<T>> {
    return this.adapter.enqueue(queue, data, options);
  }

  registerWorker<T>(queue: string,
                    handler: WorkerHandler<T>): void {
    this.adapter.registerWorker(queue, handler);
  }
}
\end{lstlisting}

\noindent
Listing~\ref{lst:g3-label-job} shows a concrete use case: shipment label generation is enqueued asynchronously with exponential backoff, decoupling the critical path (shipment creation) from the non-critical side effect (carrier API call for label generation).

\begin{lstlisting}[language=TypeScript,caption={Shipment label generation job with async decoupling (generate-label.job.ts)},label={lst:g3-label-job}]
const QUEUE_NAME = 'shipments:label-gen';

function registerLabelGenerationWorker(): void {
  const queue = QueueService.getInstance();
  queue.registerWorker<LabelGenerationData>(QUEUE_NAME,
    async (job: Job<LabelGenerationData>) => {
      console.log(`[LabelGeneration] Generating label `
        + `for shipment ${job.data.shipmentId}`);
    },
  );
}

async function enqueueLabelGeneration(
  data: LabelGenerationData,
): Promise<void> {
  const queue = QueueService.getInstance();
  await queue.enqueue(QUEUE_NAME, data, {
    attempts: 3,
    backoff: { type: 'exponential', delay: 1000 },
  });
}
\end{lstlisting}

\noindent
Because the queue infrastructure is available from the start, the team can introduce async decoupling for any inter-module interaction by defining a new queue; no infrastructure migration is required.

\paragraph*{L2: Data Isolation --- Per-Module PostgreSQL Schemas}

Schema isolation is a design decision made as part of the system's foundational design. Each module owns its data from the initial deployment, ensuring that extraction never requires data migration. The \texttt{schema-isolation.ts} module creates per-module PostgreSQL schemas and provides module-scoped \texttt{DataSource} connections with isolated \texttt{search\_path}:

\begin{lstlisting}[language=TypeScript,caption={Per-module PostgreSQL schema isolation (schema-isolation.ts)},label={lst:g3-schema-isolation}]
const MODULE_SCHEMAS = [
  'orders', 'inventory', 'payments', 'shipments'
] as const;
type ModuleName = (typeof MODULE_SCHEMAS)[number];
const moduleConnections = new Map<string, DataSource>();

async function createAllModuleSchemas(
  dataSource: DataSource,
): Promise<void> {
  if (dataSource.options.type !== 'postgres') return;
  for (const schema of MODULE_SCHEMAS) {
    await dataSource.query(
      `CREATE SCHEMA IF NOT EXISTS "${schema}"`);
  }
}

async function getModuleConnection(
  baseDataSource: DataSource,
  moduleName: ModuleName,
  entities: Function[],
): Promise<DataSource> {
  const cacheKey =
    `${moduleName}:${baseDataSource.options.database}`;
  const cached = moduleConnections.get(cacheKey);
  if (cached?.isInitialized) return cached;

  if (baseDataSource.options.type !== 'postgres') {
    moduleConnections.set(cacheKey, baseDataSource);
    return baseDataSource;
  }

  const moduleDs = new DataSource({
    ...baseDataSource.options,
    entities,
    schema: moduleName,
    extra: {
      ...(baseDataSource.options as any).extra,
      options: `-c search_path="${moduleName}",public`,
    },
  } as DataSourceOptions);

  await moduleDs.initialize();
  moduleConnections.set(cacheKey, moduleDs);
  return moduleDs;
}
\end{lstlisting}

\noindent
Each module operates within its own PostgreSQL schema from the initial \texttt{docker-compose up}. Cross-module data access is mediated through published contracts (events or API calls), never through direct table queries. When L3 extraction occurs, the module's schema migrates to a dedicated database instance---a deployment topology change, not a data migration.

\paragraph*{L3: Selective Extraction --- What Extraction Looks Like}

When $\varepsilon_{\mathrm{orders}}$ indicates that L3 is warranted, extraction is a \emph{topology change}, not a code rewrite. The domain logic, repository interfaces, and event handlers remain unchanged; what changes is how infrastructure adapters are bound and where the process runs. The following Docker Compose snippet illustrates what extracting the Orders module would look like (this configuration is illustrative; it is not yet part of the Tiny Store repository):

\begin{lstlisting}[language=yaml,caption={L3 extraction: Orders as an independent service (illustrative)},label={lst:g3-l3-extraction}]
services:
  orders-service:
    build: { context: ., dockerfile: apps/orders/Dockerfile }
    environment:
      DATABASE_URL: postgres://orders:secret@orders-db:5432/orders
      EVENT_BUS: kafka://kafka:9092
    depends_on: [orders-db, kafka]

  orders-db:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: orders
      POSTGRES_USER: orders
      POSTGRES_PASSWORD: secret

  monolith:
    build: { context: ., dockerfile: Dockerfile }
    environment:
      DATABASE_URL: postgres://app:secret@main-db:5432/tinystore
      EVENT_BUS: kafka://kafka:9092
    depends_on: [main-db, kafka]
\end{lstlisting}

\noindent
Three infrastructure changes occur at extraction, none of which touch domain logic:

\begin{enumerate}[itemsep=4pt,topsep=2pt]
  \item The \texttt{OrderRepository} adapter is re-bound to a dedicated PostgreSQL instance (\texttt{orders-db}) instead of the shared database's \texttt{orders} schema.
  \item The event bus switches from in-process dispatch to a Kafka topic, using the same event contracts defined at project start.
  \item The Orders module runs in its own container with an independent deployment pipeline.
\end{enumerate}

\noindent
L3 is presented illustratively because the operational concerns of extraction---saga orchestration, event contract versioning, deployment pipeline configuration---are the subject of G4 (Migration Readiness) and G5 (Deployment Strategy). This scoping is intentional: G3 establishes the \emph{structural preparation} that makes extraction feasible; G4 and G5 address the \emph{operational execution} that makes extraction safe. The combination ensures that by the time a team reaches L3, both the architecture and the operational tooling are ready.

Because G3's infrastructure abstractions (persistence ports, communication abstraction layer) have been present in the initial deployment, this extraction requires no changes to \texttt{OrderService}, \texttt{OrderRepository}, or any event handler. The domain logic is \emph{identical} in the extracted service and in the monolith. This is the payoff of progressive scalability: when extraction is finally warranted, it is a controlled topology change rather than a rewrite.

\paragraph*{Extraction Readiness Score}

The readiness score provides the evidence that drives progression decisions. Tiny Store includes an automated extraction readiness calculator that scores each module across five dimensions:

\begin{lstlisting}[language=TypeScript,caption={Extraction readiness scoring dimensions (extraction-readiness.ts)},label={lst:g3-readiness-score}]
// Usage: npx ts-node tools/metrics/extraction-readiness.ts
//        --module orders

const checks: CheckResult[] = [
  checkCrossModuleImports(mod),   // 25 pts
  checkEventBusUsage(mod),        // 20 pts
  checkACLLayer(mod),             // 15 pts
  checkSchemaIsolation(),         // 20 pts
  checkVersionedEvents(mod),      // 20 pts
];

const normalized = total / max;
if (normalized >= 0.80) console.log('READY for extraction');
else if (normalized >= 0.60) console.log('NEARLY READY');
else console.log('NOT READY');
\end{lstlisting}

\noindent
Example output for the Orders module:

\begin{verbatim}
Extraction Readiness Report: orders
Cross-module imports   [##########] 25/25  No cross-module imports
Event-bus usage        [##########] 20/20  listeners=yes, events=yes
ACL layer              [##########] 15/15  index.ts=yes, internal.ts=yes
Database schema iso    [##########] 20/20  schema-isolation.ts present
Versioned events       [##########] 20/20  7/7 events with version field
TOTAL: 100/100  epsilon_orders = 1.00  READY for extraction
\end{verbatim}

\noindent
The readiness score quantifies the $\varepsilon_m$ metric defined in the Metrics section. Teams run this check before any L3 extraction decision, ensuring that extraction is evidence-based rather than intuition-driven.

\subsection*{Reader Map}

The remainder of this section is organized as a hands-on tutorial that builds practical fluency with G3's scalability infrastructure and metrics. The Tutorial walks through four steps: verifying infrastructure abstraction coverage, tracing an asynchronous queue flow end-to-end, inspecting per-module PostgreSQL schema isolation, and running the extraction readiness calculator. These steps establish the practitioner's understanding of the three metric dimensions (communication flexibility via $\rho_{\mathrm{sync}}$, data ownership via $\omega$, and infrastructure abstraction via $\alpha$) before the exercises introduce controlled violations. The Exercise Walkthrough then introduces three anti-patterns, each targeting a specific G3 metric, that pass G1 and G2 gates but degrade scalability readiness. The tutorial uses all four Tiny Store modules and takes approximately 30 minutes.


\subsection*{Tutorial: Step-by-Step Application}

\subsubsection*{Step 1: Verify Infrastructure Abstractions}

G3 requires that all modules access infrastructure through the shared abstraction layer (\texttt{@tiny-store/shared-infrastructure}), never through concrete packages like \texttt{ioredis}, \texttt{bullmq}, or \texttt{pg}. This is the structural prerequisite for $\alpha = 1.0$. Verify that no module bypasses the abstraction:

\begin{lstlisting}[language=bash,caption={Check for direct infrastructure imports},label={lst:g3-tut-alpha}]
# Search for direct infrastructure imports in module source
grep -rn "from 'ioredis\|from 'bullmq\|from 'pg'" \
  libs/modules/*/src/
# Expected: no results (all access via @tiny-store/shared-infrastructure)
echo "alpha = 1.0 confirmed"
\end{lstlisting}

\subsubsection*{Step 2: Trace the Queue Flow}

Follow a shipment label-generation job from enqueue to worker to understand how L1 async decoupling works in practice. The \texttt{QueueService} provides module-scoped queues following the \texttt{module:purpose} convention. Trace the flow:

\begin{lstlisting}[language=bash,caption={Trace the label-gen queue flow},label={lst:g3-tut-queue}]
# 1. Find where the job is enqueued
grep -rn "shipments:label-gen" libs/modules/shipments/src/
# -> generate-label.job.ts: enqueueLabelGeneration()

# 2. Find where the worker is registered
grep -rn "registerWorker.*shipments:label-gen" libs/
# -> generate-label.job.ts: registerLabelGenerationWorker()

# 3. Confirm the adapter pattern (in-memory for dev, BullMQ for prod)
grep -rn "QueueAdapter" libs/shared/infrastructure/src/queue/
\end{lstlisting}

\noindent
The enqueue call and worker registration use the same \texttt{QueueService} abstraction. Switching from in-memory to BullMQ requires only an adapter swap at composition time---no business logic changes.

\subsubsection*{Step 3: Inspect Schema Isolation}

Each module owns a dedicated PostgreSQL schema. Verify this by inspecting the database directly:

\begin{lstlisting}[language=bash,caption={Verify per-module schemas in PostgreSQL},label={lst:g3-tut-schema}]
# Connect to the database and list schemas
docker exec -it tinystore-db psql -U app -d tinystore -c '\dn'
# Expected output:
#   orders    | app
#   inventory | app
#   payments  | app
#   shipments | app
#   public    | pg_database_owner

# Verify tables are in their owning module's schema
docker exec -it tinystore-db psql -U app -d tinystore \
  -c '\dt orders.* inventory.* payments.* shipments.*'
\end{lstlisting}

\noindent
Each module's tables reside exclusively within its own schema. No table appears in multiple schemas, confirming $\omega = 1.0$.

\subsubsection*{Step 4: Run the Extraction Readiness Calculator}

The extraction readiness calculator computes $\varepsilon_m$ for a given module by evaluating all five scoring dimensions. Run it against the Orders module:

\begin{lstlisting}[language=bash,caption={Compute $\varepsilon_m$ for the Orders module},label={lst:g3-tut-epsilon}]
npx ts-node tools/metrics/extraction-readiness.ts --module orders
# Expected output:
# Extraction Readiness Report: orders
# Cross-module imports   [##########] 25/25
# Event-bus usage        [##########] 20/20
# ACL layer              [##########] 15/15
# Database schema iso    [##########] 20/20
# Versioned events       [##########] 20/20
# TOTAL: 100/100  epsilon_orders = 1.00  READY for extraction
\end{lstlisting}

\noindent
The score confirms that Orders satisfies all G3 preparation criteria. Repeat for the other three modules to verify the system-wide baseline.


\pagebreak
\subsection*{Exercise Walkthrough: Controlled Violations and Signals}

This tutorial assumes the G1 and G2 baselines are passing. You will run three exercises, each introducing a scalability preparation anti-pattern that passes G1 and G2 gates but degrades G3 metrics. The exercises target the three static G3 metrics: $\rho_{\mathrm{sync}}$ (synchronous coupling), $\omega$ (data ownership), and $\alpha$ (infrastructure abstraction). Each exercise follows the same structure: introduce a single, controlled change; run the G3 gate; observe the metric regression; and apply the fix.

\subsubsection*{Exercise 0: Establish the G3 Scalability Baseline}

Run the scalability readiness gate to confirm all metrics are at their healthy $t_0$ values.

\begin{lstlisting}[language=bash,caption={G3 baseline check},label={lst:g3-ex0-baseline}]
npm run test:scalability
# All green: rho_sync=0.0, omega=1.0, alpha=1.0, epsilon>=0.8
\end{lstlisting}

\noindent
Record the baseline values: $\rho_{\mathrm{sync}} = 0.0$, $\omega = 1.0$, $\alpha = 1.0$, $\varepsilon_m \geq 0.8$ for all modules. These are the $t_0$ reference points; any degradation in subsequent exercises will be visible as a measurable regression.

\subsubsection*{Exercise 1: Synchronous Cross-Module Call ($\rho_{\mathrm{sync}}\uparrow$)}

Demonstrate that synchronous cross-module imports degrade extraction readiness even when they pass G1 and G2. Introduce the violation by modifying \texttt{libs/modules/shipments/src/features/create-shipment/service.ts} to import and call \texttt{GetPaymentHandler} from the payments module.

\begin{lstlisting}[language=TypeScript,caption={Violation: synchronous cross-module import},label={lst:g3-ex1-violation}]
/**
 * (*@\color{red}{$\times$}@*) VIOLATION: synchronous cross-module import.
 * Shipments now depends on payments at compile time.
 */
import { GetPaymentHandler } from '@tiny-store/modules-payments';

// In execute():
const payment = await this.getPaymentHandler.handle(dto.orderId);
if (payment.status !== 'processed') {
  throw new Error('Payment not confirmed');
}
\end{lstlisting}

\noindent
Why G1 and G2 pass: the import uses the public entrypoint (\texttt{@tiny-store/modules-payments}), so G1 sees no encapsulation leak. G2's maintainability metrics ($\rho_{\mathrm{api}}$, fan-in) remain within thresholds because the dependency is through a public API.

Run enforcement:

\begin{lstlisting}[language=bash,caption={Run G3 gate after Exercise 1},label={lst:g3-ex1-run}]
npm run test:scalability
\end{lstlisting}

\noindent
Expected signal: the $\rho_{\mathrm{sync}}$ test fails. The scalability gate detects a direct import from \texttt{@tiny-store/modules-payments} inside the shipments module source. $\rho_{\mathrm{sync}}$ rises from 0.0 because there is now a synchronous cross-module interaction alongside the async subscriptions.\\
Metric impact: $\rho_{\mathrm{sync}}\uparrow$, $\varepsilon_{\mathrm{shipments}}\downarrow$.\\
Fix: enrich the \texttt{OrderPaid} event payload to include payment status and ID. The shipments module receives all necessary data through the async event, eliminating the synchronous dependency.\\
Reference file: \texttt{exercises/g3/ex1-sync-payment-check.ts}

\subsubsection*{Exercise 2: Shared Table Access ($\omega\downarrow$)}

Demonstrate that cross-module data access defeats data ownership even without import violations. Introduce the violation by adding a raw SQL query in the orders module that reads from inventory's \texttt{products} table.

\begin{lstlisting}[language=TypeScript,caption={Violation: cross-module table access},label={lst:g3-ex2-violation}]
/**
 * (*@\color{red}{$\times$}@*) VIOLATION: orders reads inventory's table directly.
 */
const productRows = await this.dataSource.query(
  `SELECT "stockQuantity" FROM products WHERE sku = $1`,
  [item.sku]
);
\end{lstlisting}

\noindent
Why G1 and G2 pass: there is no import from \texttt{@tiny-store/modules-inventory}. The query is raw SQL executed through the DataSource---no boundary test detects cross-module table access. G2 sees no change in module-level dependencies.

Run enforcement:

\begin{lstlisting}[language=bash,caption={Run G3 gate after Exercise 2},label={lst:g3-ex2-run}]
npm run test:scalability
\end{lstlisting}

\noindent
Expected signal: the $\omega$ test fails. The data ownership check detects that the \texttt{products} table (owned by inventory) is referenced via SQL in the orders module source. $\omega$ drops below 1.0.\\
Metric impact: $\omega\downarrow$, $\varepsilon_{\mathrm{orders}}\downarrow$.\\
Fix: expose stock availability through an event (\texttt{InventoryUpdated} with current stock levels) or a query handler in the inventory module. The orders module should never reference another module's tables.\\
Reference file: \texttt{exercises/g3/ex2-shared-inventory-view.ts}

\subsubsection*{Exercise 3: Direct Infrastructure Dependency ($\alpha\downarrow$)}

Demonstrate that bypassing the shared infrastructure layer compromises extraction readiness. Introduce the violation by adding a direct \texttt{ioredis} import in the inventory module.

\begin{lstlisting}[language=TypeScript,caption={Violation: direct infrastructure import},label={lst:g3-ex3-violation}]
/**
 * (*@\color{red}{$\times$}@*) VIOLATION: direct infrastructure dependency.
 */
import Redis from 'ioredis';

const redis = new Redis({
  host: process.env['REDIS_HOST'] || 'localhost',
  port: parseInt(process.env['REDIS_PORT'] || '6379'),
});

// In execute():
const cached = await redis.get(`product:${productId}`);
\end{lstlisting}

\noindent
Why G1 and G2 pass: \texttt{ioredis} is a third-party npm package, not another module. G1 boundary tests only check for cross-module imports. G2 sees no maintainability regression.

Run enforcement:

\begin{lstlisting}[language=bash,caption={Run G3 gate after Exercise 3},label={lst:g3-ex3-run}]
npm run test:scalability
\end{lstlisting}

\noindent
Expected signal: the $\alpha$ test fails. The infrastructure abstraction check detects a direct import of \texttt{ioredis} in the inventory module source. $\alpha$ drops below 1.0.\\
Metric impact: $\alpha\downarrow$, $\varepsilon_{\mathrm{inventory}}\downarrow$.\\
Fix: use \texttt{CacheService} from \texttt{@tiny-store/shared-infrastructure} instead of importing Redis directly.\\
Reference file: \texttt{exercises/g3/ex3-direct-redis-import.ts}

\subsubsection*{Exercise Summary}

\small
\noindent
\begin{tabularx}{\linewidth}{p{0.06\linewidth} X p{0.24\linewidth} p{0.20\linewidth}}
\textbf{Ex.} & \textbf{What you change} & \textbf{Run} & \textbf{Signal / metric} \\
0 & Establish G3 baseline & \texttt{test:scalability} & pass (baseline) \\
1 & Sync cross-module import in shipments & \texttt{test:scalability} & fail, $\rho_{\mathrm{sync}}\uparrow$ \\
2 & Raw SQL to inventory's table from orders & \texttt{test:scalability} & fail, $\omega\downarrow$ \\
3 & Direct \texttt{ioredis} import in inventory & \texttt{test:scalability} & fail, $\alpha\downarrow$ \\
\end{tabularx}
\normalsize

\subsubsection*{New Metrics: Tiny Store Baseline}

With the infrastructure in place, the G3 metrics can be computed from the current Tiny Store state to establish the $t_0$ reference for longitudinal tracking.

\emph{Spectrum position:} All four modules currently operate at L0 (vertical optimization). No module has triggered the L1 threshold because inter-module communication is already event-driven as part of the system's foundational design, and no throughput divergence has been observed under the current test workload.

\emph{Note on runtime metrics:} Module Throughput Profile ($\Theta_m$) and Module Latency Variance ($\sigma^2_{L,m}$) are runtime metrics whose $t_0$ baselines are established through load testing rather than static code analysis. The static metrics below ($\rho_{\mathrm{sync}}$, $\omega$, $\alpha$, $\varepsilon_m$) can be computed from the codebase at any point.

\emph{Cross-Module Synchronous Call Ratio ($\rho_{\mathrm{sync}}$):} All inter-module interactions in the composition root flow through \texttt{eventBus.subscribe}, with zero synchronous cross-module calls at the library level.

\begin{lstlisting}[language=bash,caption={$\rho_{\mathrm{sync}}$ computation},label={lst:g3-rho-sync}]
rho_sync = 0 / (0 + N) = 0.0  % N = total event subscriptions
\end{lstlisting}

\noindent
A value of 0.0 confirms that all inter-module communication is asynchronous, well below the 0.3 threshold.

\emph{Data Ownership Clarity Index ($\omega$):} Tiny Store defines 5 entities across 4 modules (Orders: 1, Inventory: 2, Payments: 1, Shipments: 1). Each entity resides exclusively within its owning module's schema.

\begin{lstlisting}[language=bash,caption={$\omega$ computation},label={lst:g3-omega}]
omega = 5 / 5 = 1.0
\end{lstlisting}

\emph{Infrastructure Abstraction Coverage ($\alpha$):} All modules access infrastructure through the \texttt{@tiny-store/shared-infrastructure} layer (cache, queue, database, event bus).

\begin{lstlisting}[language=bash,caption={$\alpha$ computation},label={lst:g3-alpha}]
alpha = 1.0
\end{lstlisting}

\emph{Extraction Readiness Score ($\varepsilon_m$):} Running the readiness calculator on all modules yields 100/100, with all five scoring dimensions fully satisfied since events now include explicit version fields.

\begin{lstlisting}[language=bash,caption={$\varepsilon_m$ for all modules},label={lst:g3-epsilon}]
epsilon_orders    = 100 / 100 = 1.00
epsilon_inventory = 100 / 100 = 1.00
epsilon_payments  = 100 / 100 = 1.00
epsilon_shipments = 100 / 100 = 1.00
\end{lstlisting}

\vspace{0.5em}
\noindent
All G3 metrics are at healthy baseline levels: $\rho_{\mathrm{sync}} = 0.0$ (fully async), $\omega = 1.0$ (clear data ownership), $\alpha = 1.0$ (fully abstracted), $\varepsilon_m = 1.0$ (extraction-ready for all modules). These values confirm that the Tiny Store architecture satisfies G3 compliance as established at project start. The metrics establish the $t_0$ reference: any future degradation (a new synchronous cross-module call, a shared table, a direct infrastructure dependency) will be visible as a measurable regression against this baseline.

\vspace{0.5em}
\noindent
The exercise walkthrough confirms that G3 enforcement catches scalability preparation regressions that G1 and G2 leave undetected. Each violation---synchronous coupling, shared data access, direct infrastructure dependency---would be invisible to boundary and maintainability gates alone. The G3 scalability readiness tests make these degradations measurable and actionable, preserving the option to extract modules independently as the system grows.


\subsection*{Conclusion of the G3 Implementation}

This section demonstrated that scalability in a modular monolith is not a binary property that requires distribution to achieve, but a gradient that can be traversed through deliberate, proportional interventions. G1 ensures that module boundaries are correct; G2 ensures that they remain sustainable over time; G3 ensures that the resulting modular structure can absorb growth without requiring a wholesale architectural transformation.

The progressive scalability spectrum (L0--L3) provides a concrete decision framework: vertical optimization before async decoupling, async decoupling before data isolation, data isolation before extraction. Each level is triggered by observable metrics rather than speculative anticipation, and each intervention is reversible if conditions change. The Tiny Store reference implementation demonstrates that the infrastructure for all four levels (caching with adapter pattern, module-scoped queues, per-module PostgreSQL schemas, extraction readiness scoring) can be embedded from the first deployment, making scalability a structural property of the architecture rather than a future migration project.

With boundaries enforced (G1), maintainability preserved (G2), and scalability infrastructure prepared (G3), the architecture is structurally ready for growth. However, when a module's extraction readiness score indicates that L3 is warranted, the team needs a systematic process for preparing that module for independent deployment. G4 addresses this concern by defining the migration readiness criteria, event-driven communication contracts, and saga orchestration patterns that transform extraction from an emergency response into a controlled, evidence-based topology change.
