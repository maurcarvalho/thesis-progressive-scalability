\label{sec:background}
This chapter presents the technical foundations and architectural paradigms relevant to the research questions explored in this research. It focuses on the principles and trade-offs associated with cloud-native applications, microservices, monolithic architectures, the software industry's open-source philosophy, and system modularity, each of which plays a significant role in shaping how software systems are built, maintained, and scaled in contemporary environments.

Rather than prescribing a single correct approach, this chapter aims to clarify how each paradigm addresses (or fails to address) the practical constraints faced in early-stage system development. By analyzing these foundations in depth, the chapter sets the stage for the literature review and the architectural guidelines proposed in later sections.


\section{Cloud-Native Applications}

Cloud-native applications are software systems explicitly designed to take full advantage of cloud computing environments. Rather than treating cloud infrastructure as a deployment target, these systems are built to operate \textit{\enquote{natively}} in the cloud, leveraging its inherent characteristics such as elasticity, horizontal scalability, fault tolerance, and automation. As described by Fowler \cite{fowler2020}, cloud-native systems embrace the principles of disposability, resilience, and automation from their inception, allowing teams to ship faster, recover from failures more effectively, and scale seamlessly as demand grows.

Several enabling technologies and practices are commonly associated with the cloud-native paradigm. Among them, containerization (e.g., Docker) allows developers to encapsulate application logic and dependencies in lightweight, portable units. These containers are typically orchestrated using platforms such as Kubernetes, which provide mechanisms for service discovery, auto-scaling, and fault isolation. Infrastructure as Code (IaC), through tools like Terraform or AWS CloudFormation, further promotes repeatability, version control, and operational efficiency by allowing infrastructure to be provisioned programmatically. Continuous Integration and Continuous Deployment (CI/CD) pipelines support rapid iteration and deployment, helping organizations maintain delivery velocity without compromising stability.

However, cloud-native is more than a technological stack; it is a design philosophy. The architecture of cloud-native applications is typically service-oriented or event-driven, emphasizing decoupled components, stateless services, and fault-tolerant boundaries. Systems are expected to degrade gracefully, recover autonomously, and scale independently. These properties demand a rethinking of traditional monolithic architectures, which are often tightly coupled and optimized for static environments.

In the context of early-stage startups, cloud-native principles offer clear benefits: faster time-to-market, infrastructure scalability, and alignment with modern DevOps practices. Nonetheless, they also introduce complexity. Designing a system that is both cloud-native and maintainable requires thoughtful abstraction, modular boundaries, and operational maturity that many small teams may not yet possess. The tension between speed and structure is particularly difficult during the initial phases of product engineering, when the pressure to deliver may overshadow long-term architectural considerations.

This backdrop sets the stage for exploring how different architectural paradigms such as microservices, monoliths, and modular monoliths respond to the demands of cloud-native environments. 

\section{Microservices Architecture (MSA)}

Microservices architecture (MSA) is a widely adopted paradigm in cloud-native development. It structures applications as a collection of small, autonomous services, each encapsulating a specific business capability. These services interact through lightweight communication mechanisms, typically RESTful APIs for synchronous interactions or asynchronous messaging for decoupled workflows. Common tools supporting asynchronous communication include Apache Kafka, RabbitMQ, and cloud-native solutions such as Amazon SQS and Google Cloud Pub/Sub. This architectural style enables independent deployment of services, allowing for isolated development, testing, and release cycles that promote agility and scalability.


This design promotes modularity by enforcing service boundaries and encourages team autonomy, as individual teams can own and operate distinct services. The architecture also enables horizontal scalability, since services can be scaled independently based on demand. Fowler and Lewis argue that this decomposition fosters organizational agility and technological flexibility by reducing interdependencies between teams and promoting continuous delivery pipelines~\cite{fowler2014}. Academic literature further formalizes the microservices paradigm through key principles such as bounded contexts, decentralized governance, and independent deployment units~\cite{dragoni2017microservices, taibi2018definition}.

Despite its appeal, microservices also introduce considerable complexity. The benefits of service isolation come at the cost of increased operational overhead, including the need for distributed tracing, inter-service coordination, deployment orchestration, and resilience strategies. Designing systems in this manner often requires mature DevOps practices and advanced infrastructure automation, which are not always feasible in early-stage environments. Studies show that premature adoption of microservices can lead to over-engineering, performance bottlenecks, and cognitive overload for development teams~\cite{fritzsch2019, gysel2016service}.

Moreover, the theoretical ideals of microservices do not always align with real-world implementations. While principles such as bounded context and single responsibility are conceptually appealing, their translation into clear service boundaries often proves difficult. This disconnect between theory and practice has led to architectural inconsistencies, accidental complexity, and, in some cases, architectural degradation over time~\cite{taibi2018definition, deLauretis2019from}.

In light of these limitations, several studies have begun to reevaluate microservices as a default choice for all scenarios. Particularly in the context of startups and small teams, the microservices approach may introduce more challenges than benefits, as comparatively analyzed by Hmue et al.~\cite{hmue2024microservices}. This critique opens the door for alternative strategies that aim to preserve the internal modularity and scalability potential of microservices while minimizing their early-stage overhead.

\section{Monolithic Systems}

In contrast, monolithic systems combine user interfaces, business logic, and data access layers---often described as the presentation-domain-data layering pattern~\cite{fowler2015presentationdomaindatalayering}---into a single deployable unit. Although monoliths are often simpler, easier to develop, debug, test, and deploy during the early stages of product development ~\cite{richardson2018microservices}, they can become difficult to evolve and scale as complexity increases. Fowler argues that beginning with a well-structured monolith is frequently the most pragmatic approach, reserving the transition to microservices for when system demands necessitate it~\cite{fowler2015}, however, the challenge lies precisely in creating a monolith that is truly modular from the start. Without clear internal boundaries, many monoliths become tightly coupled and resistant to change, making future decomposition into services a costly and error-prone process. This difficulty is well-documented in the literature, where decomposition of legacy monoliths is described as one of the most complex and risky undertakings in software modernization ~\cite{alshuqayran2016systematic}.

\section{The Centrality of Modularity}

Modularity is a foundational design principle that supports software evolution, regardless of the architectural style adopted. It describes the degree to which a system’s internal components can be isolated, composed, and recombined without introducing excessive coupling. While its theoretical benefits are widely endorsed, practical implementation, particularly in monolithic systems, often falls short. Modularity is frequently assumed rather than engineered, resulting in architectures that degrade over time.

In the startup context, modularity is not an optional refinement; it is a prerequisite for survival. It enables small teams to iterate quickly, isolate change, and adapt architectural boundaries as the product evolves. By enforcing internal boundaries early, teams can defer decisions about distributed systems until they are truly warranted. However, most actionable guidance on modularity is either embedded within microservices literature. For monolithic systems under real-world constraints, especially in early-stage environments, there is a noticeable lack of operationalized strategies.

More precisely, coupling and cohesion are the two fundamental metrics that determine the quality of modular boundaries. \textit{Coupling} measures the degree of interdependence between modules. Afferent coupling (Ca) counts the number of external modules that depend on a given module, while efferent coupling (Ce) counts the number of external modules on which that module depends. High efferent coupling signals that a module is overly reliant on others, making it fragile to change; high afferent coupling indicates that a module is widely depended upon, meaning changes to it carry high blast radius~\cite{martin2012clean}. \textit{Cohesion}, conversely, measures how strongly related the responsibilities within a single module are. Functional cohesion---the strongest form---occurs when every element in a module contributes to a single, well-defined task. Sequential cohesion arises when the output of one element serves as input to the next. Communicational cohesion groups elements that operate on the same data. Designing for high cohesion and low coupling is the foundational heuristic behind guidelines G1 and G2 in Chapter~\ref{sec:guidelines}.

Architectural decisions made in the early stages of product development often have long-term consequences. Startups frequently fall into one of two traps: prematurely adopting microservices and incurring unnecessary complexity, or building monoliths that become so tightly coupled that scaling requires expensive rewrites or risky migrations. These failures often stem not from the choice of architecture itself, but from neglecting modularity at the outset.

As Stefan Tilkov cautions, the assumption that microservices can be easily extracted from a well-written monolith is deeply flawed. In practice, monoliths tend to accumulate shared abstractions, leaky boundaries, and tightly bound processes~\cite{tilkov2015}. Without deliberate modular design, decomposition becomes not only difficult but disruptive, undermining business continuity and increasing technical debt.

\begin{figure}[h!]
\centering
\includegraphics[width=0.7\linewidth]{Cap1/monolith-theory-practice.png}
\caption{The contrast between theory and practice when designing applications for modularity}
\label{fig:monolith-theory-vs-practice}
\end{figure}

\section{Domain-Driven Design and Bounded Contexts}

Domain-Driven Design (DDD), as introduced by Eric Evans~\cite{evans2003ddd}, is a software design approach that places the business domain at the center of architectural decisions. Rather than organizing systems around technical concerns, DDD advocates for modeling software structures that reflect the domain they serve, ensuring that the codebase evolves in alignment with business understanding.

At the strategic level, DDD introduces the concept of \textit{bounded contexts}: explicitly delimited areas of the domain in which a particular model applies consistently. Each bounded context defines its own ubiquitous language---a shared vocabulary between developers and domain experts that eliminates ambiguity within that boundary. When a term such as ``order'' carries different meanings in fulfillment and billing, each context maintains its own definition, preventing model corruption across boundaries.

Bounded contexts are the primary tool for modular decomposition in DDD. In a modular monolith, each bounded context maps naturally to a module: the context boundary becomes the module boundary, the ubiquitous language becomes the module's internal API vocabulary, and cross-context relationships are mediated through well-defined integration points. Strategic DDD further provides \textit{context maps}, which document how bounded contexts relate to one another---through shared kernels, anti-corruption layers, customer-supplier relationships, or published languages. These maps serve as architectural blueprints for inter-module communication.

The tactical side of DDD---aggregates, entities, value objects, domain events, and repositories---provides the internal building blocks within each bounded context. While tactical patterns are implementation-level concerns, they reinforce modularity by ensuring that state mutations are coordinated through aggregate roots and that domain events serve as the primary mechanism for cross-module communication, a principle directly operationalized in guidelines G1 and G4 of this research.

The distinction between strategic and tactical DDD is critical for startups. Teams can adopt strategic DDD (bounded contexts, context maps, ubiquitous language) to establish sound modular boundaries without committing to the full tactical pattern catalog, which may introduce unnecessary complexity in early-stage systems. This selective adoption aligns with the progressive scalability philosophy advocated throughout this dissertation.

\section{Architectural Patterns for Modular Systems}

Several architectural patterns have been proposed to organize the internal structure of software modules in ways that promote separation of concerns and dependency management. Understanding these patterns is essential for designing modular monoliths that remain maintainable as they grow.

The \textit{layered architecture} is the most traditional approach, organizing code into horizontal layers---typically presentation, business logic, and data access. Each layer depends only on the layer directly below it. While simple and widely understood, layered architectures tend to encourage package-by-layer organization, where technical concerns (controllers, services, repositories) take precedence over domain boundaries. This structure can obscure business intent and create implicit coupling between unrelated domain concepts that share the same layer~\cite{martin2012clean}.

The \textit{hexagonal architecture} (ports and adapters), introduced by Alistair Cockburn~\cite{cockburn2005hexagonal}, inverts the dependency direction. The application core defines \textit{ports}---interfaces that describe how it interacts with the outside world---while \textit{adapters} implement these interfaces for specific technologies (databases, HTTP, messaging). This pattern ensures that the domain logic has no compile-time dependency on infrastructure, making modules independently testable and technology-agnostic.

Robert C. Martin's \textit{clean architecture}~\cite{martin2012clean} generalizes this principle into concentric layers where dependencies point inward: entities at the center, surrounded by use cases, interface adapters, and frameworks. The dependency rule---source code dependencies must point only inward---ensures that the most stable, business-critical code is insulated from volatile external concerns.

These patterns directly support modular monolith design. When each module adopts hexagonal or clean architecture internally, inter-module dependencies are mediated through explicit ports rather than shared implementation details. This structure enables \textit{package-by-feature} organization, where top-level directories represent business capabilities (orders, payments, inventory) rather than technical layers. Package-by-feature naturally aligns with bounded contexts and makes the architecture ``scream'' its business purpose---a quality that guidelines G1 through G3 formalize and operationalize.

\section{Modular Monoliths: A Foundational Overview}

A \textit{modular monolith} is an architectural style in which a software system is deployed as a single unit (monolith), but is internally structured around clearly defined, independently evolvable and testable modules. Each module encapsulates a distinct domain or functional responsibility, as examined by Su and Li~\cite{su2024modular}, enforcing boundaries through practices such as internal APIs or class interfaces, strict dependency management, and clear ownership. While the system runs as a unified process, its internal modularity facilitates separation of concerns, testability, and potential future decomposition into services.

Unlike traditional monoliths, which often evolve into tightly coupled systems, modular monoliths are designed with scalability and maintainability in mind from the outset. They aim to combine the simplicity of monolithic deployment with the flexibility of modular design, offering a pragmatic path for startups and evolving teams who seek to delay the complexity of distributed systems without sacrificing long-term adaptability.

Although the terminology around modular monoliths is still maturing and can be abstract, many engineers who end up with a system that are full of failures and bugs, making it extremely hard to maintain or include new features, originally aimed to build clean, modular, bug free and extensible systems. This disconnect between design intention and architectural outcome highlights the importance of understanding which structural principles and architectural decisions help preserve modularity over time and prepare systems for eventual distribution.


To address this challenge, a growing body of literature and real-world experience suggests that monolithic systems, when thoughtfully designed, can still be effectively deployed as cloud-native applications. Despite this, industry practice continues to lean heavily toward microservices adoption, even when foundational design principles of Microservices such as the First Law of Distributed Objects being \textit{“Don’t distribute your system!”} \cite{fowler2002} are ignored. This trend is not new. As Fred Brooks famously warned, developers often over-design their \textit{“second systems”} to avoid the pain points of the first \cite{brooks1995}. In doing so, they frequently introduce complexity that is not aligned with current needs, trying to avoid past errors and usually landing in a place full of unknown challenges.

Instead of defaulting to distributed systems, startups benefit more from critically evaluating their context and choosing simpler, scalable solutions that suit their current stage of growth. This is where modular monolithic architectures emerge as a powerful alternative because they offer startups a pragmatic pathway by supporting the progressive extraction of services, based on real demand instead of future needs speculation \cite{celozzi2020}.

\section{Modular Monoliths: Case Studies}

Modular Monoliths 
Recent case studies highlight the pragmatic turn toward this architecture. Segment initially adopted microservices to enable rapid parallel development but faced operational overhead and latency issues. By consolidating into a modular monolith, they reduced complexity and improved developer productivity \cite{segment2023}. Similarly, Auth0 and Medium migrated specific services back into monolithic architectures to alleviate development friction and better manage resources \cite{auth02019, medium2019}. Amazon Prime Video followed a similar path, reporting performance and cost improvements of up to 90\% after consolidating certain microservices \cite{primevideo2023}.

Shopify, by contrast, never fully transitioned to microservices. Instead, it progressively evolved its monolithic Ruby on Rails codebase into a modular monolith, preserving developer velocity while improving scalability and boundary enforcement \cite{shopify2022}.

These examples demonstrate that modular monoliths allow startups and even big organizations to maintain system coherence and engineering velocity, while deferring the complexities of distributed architectures until they are truly needed, a trend also surveyed by Al-Qora'n and Al-Said Ahmad~\cite{alqoran2025mma}. As Thoughtworks notes, this architecture simplifies deployment, reduces inter-service communication overhead, and supports straightforward CI/CD pipelines \cite{garg2023}. If performance or scaling needs arise, specific modules can be independently
deployed as microservices, without requiring a total rewrite of the codebase.
\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{Cap1/inline_mod_mono_01.png}
\caption{An example of modules that can be independently deployed when the need arises}
\label{fig:example-image}
\end{figure}

Development within a unified codebase lowers onboarding costs and reduces the cognitive overhead of managing multiple repositories, services, and infrastructure stacks. 

Importantly, the modular monolith paradigm aligns with long-standing architectural principles such as YAGNI \textit{“You Aren’t Gonna Need It”} \cite{extremeProgramming} and evolutionary software architecture \cite{FordParsons2017}. These principles advocate for designing systems incrementally, delivering only what is essential at each stage, and deliberately postponing irreversible architectural commitments until they are validated by concrete operational needs. This approach is not new in software engineering; rather, it reflects a recurring theme that resurfaces as technological landscapes, computational constraints, and business objectives evolve. Throughout the history of software architecture, the tension between simplicity and extensibility has repeatedly driven practitioners to reassess the trade-offs between monolithic and distributed designs. While tools, platforms, and deployment models may change, the fundamental challenge of balancing flexibility, maintainability, and scalability in early system design remains constant.

In this light, modular monolithic architecture addresses many of the persistent shortcomings associated with both traditional monoliths and microservices. By enforcing explicit module boundaries, it prevents the erosion of structure often seen in large monolithic systems. Without such discipline, codebases can devolve into what Foote and Yoder termed a \textit{“big ball of mud”}, a system characterized by high complexity and poor modularization \cite{FooteYoder1999}. Proper boundary enforcement ensures that changes within one module have limited and well-defined effects on others, thereby reducing the risk of unintended consequences and improving system robustness.

Beyond maintainability, performance is another domain where modular monoliths offer clear benefits. Unlike microservices, which require inter-process and often networked communication, modular monoliths rely on intra-process method calls. These calls are significantly faster and less error-prone than their distributed counterparts, leading to more responsive applications and lower infrastructure costs related to communication overhead.

Testing workflows also benefit substantially from a modular monolith design. Integration and end-to-end testing become more straightforward within a single deployable unit. Developers can replicate production logic on local machines without complex orchestration layers, enabling faster feedback cycles and more reliable test coverage. 

Despite sound technical arguments for modular monoliths and other architectures, real-world decisions are often driven more by market trends than critical evaluation. Influential companies and open-source ecosystems frequently shape architectural choices, leading teams to adopt microservices prematurely. Understanding this dynamic requires examining how industry players define and disseminate prevailing architectural paradigms.



\section{Software Industry}
This research defines the software industry as comprising engineering practitioners, providers, and suppliers engaged in the development of cloud-native systems. Within this domain, large technology companies exert significant influence, not only through market presence but also via open-source initiatives that shape architectural trends and tooling.

Startups in this industry frequently adopt agile methodologies and form small, autonomous teams. These structures, while optimized for rapid iteration, often influence software design in ways that reflect internal communication patterns. According to Conway's Law, organizations tend to produce systems whose structure mirrors their own communication processes~\cite{conway2003, fowler2003}. Brooks further observes that initial system designs frequently replicate the company's early organizational chart, which rarely withstands future growth or change \cite{brooks1995}. This tendency may result in rigid or suboptimal system boundaries, reinforcing technical debt and reducing scalability.

As a result, mimicking the architectures and technical practices of giant tech companies such as Facebook, Amazon, Netflix, Google, and Microsoft can lead to poor outcomes when applied to early-stage startups. These companies operate at fundamentally different scales, and their engineering decisions are optimized for high-load environments that few startups initially face. Yet, the influence of their open-source tools, infrastructure libraries, and technical content trougth talks and engineering blogs is pervasive. According to the Open Source Contributor Index (OSCI), Google and Microsoft are currently the top two contributors to open source software in the entire industry \cite{osci2024}, shaping much of the modern tooling landscape and practices.

This open source leadership reinforces the perception that tools developed by large tech firms represent best practices that should be adopted universally. The prestige, community support, and detailed documentation surrounding these technologies create strong incentives for adoption. As a result, many startups incorporate complex cloud-native tools without assessing whether their system requirements justify such choices. The marketing and advocacy strategies of big-tech companies further amplify this effect by framing their technologies as solutions for performance and scalability challenges that may not yet apply to early-stage startups.

This ecosystem cultivates a form of architectural path dependency, where decisions are driven more by perceived legitimacy than by contextual fit. In particular, microservices are often adopted not as a strategic response to scaling needs but as a reaction to industry trends and negative past experiences with poorly structured monoliths. Premature adoption of distributed patterns frequently leads to over-engineering, unnecessary abstraction layers, and inefficient resource allocation. In many cloud-native systems, the shift toward microservices reflects a retrospective corrective rather than a proactive, context-aware decision.

Academic literature frequently reinforces this narrative by framing monolithic architectures primarily as legacy systems in need of migration~\cite{fritzsch2019, kazanavicius2019, jin2021}. While such studies contribute valuable insights into transformation techniques, they tend to overlook the architectural potential of well-designed monoliths, particularly when structured with modularity in mind.

This prevailing bias under-represents an important alternative: that monolithic systems, when built with clear internal boundaries and a disciplined modular structure, can offer maintainability, scalability, and developer efficiency comparable to microservices and do that without their operational overhead. As the next chapters will explore, modular monoliths may serve not only as transitional architectures but as sustainable long-term solutions for startups navigating uncertain growth trajectories and limited resources. Understanding this possibility requires revisiting architectural assumptions and establishing practical guidelines grounded in empirical evidence and contextual needs.
